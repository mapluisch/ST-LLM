{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0AQMFUzbfE-e",
        "outputId": "235d58aa-3207-4355-d268-2cd18e14519c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ST-LLM'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 179 (delta 43), reused 148 (delta 25), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (179/179), 19.42 MiB | 24.52 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/ST-LLM\n",
            "Collecting torch==2.0.0 (from -r requirement.txt (line 1))\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.1 (from -r requirement.txt (line 2))\n",
            "  Downloading torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.1 (from -r requirement.txt (line 3))\n",
            "  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r requirement.txt (line 4))\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp==3.8.4 (from -r requirement.txt (line 5))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirement.txt (line 6)) (1.3.1)\n",
            "Collecting async-timeout==4.0.2 (from -r requirement.txt (line 7))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting attrs==22.2.0 (from -r requirement.txt (line 8))\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.37.0 (from -r requirement.txt (line 9))\n",
            "  Downloading bitsandbytes-0.37.0-py3-none-any.whl (76.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cchardet==2.1.7 (from -r requirement.txt (line 10))\n",
            "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chardet==5.1.0 (from -r requirement.txt (line 11))\n",
            "  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contourpy==1.0.7 (from -r requirement.txt (line 12))\n",
            "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.11.0 (from -r requirement.txt (line 13))\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting filelock==3.9.0 (from -r requirement.txt (line 14))\n",
            "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting fonttools==4.38.0 (from -r requirement.txt (line 15))\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist==1.3.3 (from -r requirement.txt (line 16))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub==0.17.0 (from -r requirement.txt (line 17))\n",
            "  Downloading huggingface_hub-0.17.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-resources==5.12.0 (from -r requirement.txt (line 18))\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Collecting kiwisolver==1.4.4 (from -r requirement.txt (line 19))\n",
            "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.7.0 (from -r requirement.txt (line 20))\n",
            "  Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict==6.0.4 (from -r requirement.txt (line 21))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.27.0 (from -r requirement.txt (line 22))\n",
            "  Downloading openai-0.27.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==23.0 (from -r requirement.txt (line 23))\n",
            "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil==5.9.4 (from -r requirement.txt (line 24))\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycocotools==2.0.6 (from -r requirement.txt (line 25))\n",
            "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyparsing==3.0.9 (from -r requirement.txt (line 26))\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirement.txt (line 27)) (2.8.2)\n",
            "Collecting pyyaml==6.0 (from -r requirement.txt (line 28))\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex==2022.10.31 (from -r requirement.txt (line 29))\n",
            "  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.13.2 (from -r requirement.txt (line 30))\n",
            "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.1 (from -r requirement.txt (line 31))\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.28.0 (from -r requirement.txt (line 32))\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.6.13 (from -r requirement.txt (line 33))\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==3.5.1 (from -r requirement.txt (line 34))\n",
            "  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset==0.2.48 (from -r requirement.txt (line 35))\n",
            "  Downloading webdataset-0.2.48-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirement.txt (line 36)) (1.2.2)\n",
            "Collecting scipy==1.10.1 (from -r requirement.txt (line 37))\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl==1.8.2 (from -r requirement.txt (line 38))\n",
            "  Downloading yarl-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zipp==3.14.0 (from -r requirement.txt (line 39))\n",
            "  Downloading zipp-3.14.0-py3-none-any.whl (6.7 kB)\n",
            "Collecting omegaconf==2.3.0 (from -r requirement.txt (line 40))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.7.0.72 (from -r requirement.txt (line 41))\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iopath==0.1.10 (from -r requirement.txt (line 42))\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting decord==0.6.0 (from -r requirement.txt (line 43))\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tenacity==8.2.2 (from -r requirement.txt (line 44))\n",
            "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
            "Collecting pycocoevalcap (from -r requirement.txt (line 45))\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from -r requirement.txt (line 46))\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting umap-learn (from -r requirement.txt (line 47))\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from -r requirement.txt (line 48)) (6.5.5)\n",
            "Collecting gradio==3.24.1 (from -r requirement.txt (line 49))\n",
            "  Downloading gradio-3.24.1-py3-none-any.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio-client==0.0.8 (from -r requirement.txt (line 50))\n",
            "  Downloading gradio_client-0.0.8-py3-none-any.whl (20 kB)\n",
            "Collecting wandb (from -r requirement.txt (line 51))\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.8.1 (from -r requirement.txt (line 52))\n",
            "  Downloading peft-0.8.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.7.0 (from -r requirement.txt (line 53))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio==2.33.1 (from -r requirement.txt (line 54))\n",
            "  Downloading imageio-2.33.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av==11.0.0 (from -r requirement.txt (line 55))\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[deepspeed] in /usr/local/lib/python3.10/dist-packages (from -r requirement.txt (line 56)) (4.40.0)\n",
            "Collecting mmengine (from -r requirement.txt (line 57))\n",
            "  Downloading mmengine-0.10.4-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers (from -r requirement.txt (line 58))\n",
            "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirement.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirement.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirement.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirement.txt (line 1)) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1->-r requirement.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1->-r requirement.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1->-r requirement.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.4->-r requirement.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.0->-r requirement.txt (line 17)) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2->-r requirement.txt (line 27)) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.8 (from spacy==3.5.1->-r requirement.txt (line 34))\n",
            "  Downloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (919 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.6/919.6 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (2.0.10)\n",
            "Collecting typer<0.8.0,>=0.3.0 (from spacy==3.5.1->-r requirement.txt (line 34))\n",
            "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Collecting pathy>=0.10.0 (from spacy==3.5.1->-r requirement.txt (line 34))\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (6.4.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy==3.5.1->-r requirement.txt (line 34))\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r requirement.txt (line 34)) (3.3.0)\n",
            "Collecting braceexpand (from webdataset==0.2.48->-r requirement.txt (line 35))\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->-r requirement.txt (line 36)) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->-r requirement.txt (line 36)) (3.4.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl==1.8.2->-r requirement.txt (line 38)) (3.7)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf==2.3.0->-r requirement.txt (line 40))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath==0.1.10->-r requirement.txt (line 42))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting aiofiles (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.24.1->-r requirement.txt (line 49)) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading fastapi-0.110.2-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.24.1->-r requirement.txt (line 49)) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio==3.24.1->-r requirement.txt (line 49)) (2.1.5)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio==3.24.1->-r requirement.txt (line 49)) (2.0.3)\n",
            "Collecting pydub (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting semantic-version (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.8.1->-r requirement.txt (line 52)) (0.4.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirement.txt (line 1)) (0.43.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirement.txt (line 1)) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->-r requirement.txt (line 1))\n",
            "  Downloading lit-18.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sentence-transformers (from -r requirement.txt (line 46))\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sentence_transformers-2.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sentence_transformers-2.5.0-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sentence_transformers-2.3.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirement.txt (line 46)) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirement.txt (line 46)) (0.1.99)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn->-r requirement.txt (line 47)) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn->-r requirement.txt (line 47))\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (6.3.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (23.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (5.7.2)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (1.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (5.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirement.txt (line 48)) (1.0.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirement.txt (line 51)) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirement.txt (line 51))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->-r requirement.txt (line 51))\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirement.txt (line 51))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb->-r requirement.txt (line 51))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirement.txt (line 51)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirement.txt (line 51)) (3.20.3)\n",
            "INFO: pip is looking at multiple versions of transformers[deepspeed] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers[deepspeed] (from -r requirement.txt (line 56))\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of transformers[deepspeed] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.1-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed>=0.8.3 (from transformers==4.28.0->-r requirement.txt (line 32))\n",
            "  Downloading deepspeed-0.14.2.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from mmengine->-r requirement.txt (line 57))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine->-r requirement.txt (line 57)) (13.7.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine->-r requirement.txt (line 57)) (2.4.0)\n",
            "Collecting yapf (from mmengine->-r requirement.txt (line 57))\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting xformers (from -r requirement.txt (line 58))\n",
            "  Downloading xformers-0.0.25-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading xformers-0.0.23-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl (211.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyre-extensions==0.0.29 (from xformers->-r requirement.txt (line 58))\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Collecting xformers (from -r requirement.txt (line 58))\n",
            "  Downloading xformers-0.0.19-cp310-cp310-manylinux2014_x86_64.whl (108.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect (from pyre-extensions==0.0.29->xformers->-r requirement.txt (line 58))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.24.1->-r requirement.txt (line 49)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.24.1->-r requirement.txt (line 49)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.24.1->-r requirement.txt (line 49)) (0.12.1)\n",
            "Collecting hjson (from deepspeed>=0.8.3->transformers==4.28.0->-r requirement.txt (line 32))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed>=0.8.3->transformers==4.28.0->-r requirement.txt (line 32))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.8.3->transformers==4.28.0->-r requirement.txt (line 32)) (9.0.0)\n",
            "Collecting pynvml (from deepspeed>=0.8.3->transformers==4.28.0->-r requirement.txt (line 32))\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirement.txt (line 51))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook->-r requirement.txt (line 48)) (4.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.24.1->-r requirement.txt (line 49)) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.24.1->-r requirement.txt (line 49)) (2.0.3)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->-r requirement.txt (line 48)) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->-r requirement.txt (line 48)) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook->-r requirement.txt (line 48)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->-r requirement.txt (line 48)) (2.19.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn->-r requirement.txt (line 47)) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.24.1->-r requirement.txt (line 49)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.24.1->-r requirement.txt (line 49)) (2024.1)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.10.0->spacy==3.5.1->-r requirement.txt (line 34))\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1->-r requirement.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1->-r requirement.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->-r requirement.txt (line 48)) (0.7.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r requirement.txt (line 34)) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r requirement.txt (line 34)) (0.1.4)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->-r requirement.txt (line 48)) (21.2.0)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.24.1->-r requirement.txt (line 49)) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.24.1->-r requirement.txt (line 49)) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->gradio==3.24.1->-r requirement.txt (line 49))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->notebook->-r requirement.txt (line 48)) (7.34.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->-r requirement.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine->-r requirement.txt (line 57)) (7.1.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine->-r requirement.txt (line 57)) (2.0.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirement.txt (line 51))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (3.0.43)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (4.9.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.24.1->-r requirement.txt (line 49)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.24.1->-r requirement.txt (line 49)) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.24.1->-r requirement.txt (line 49)) (0.18.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->-r requirement.txt (line 48)) (1.7.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.24.1->-r requirement.txt (line 49)) (1.2.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.24.1->-r requirement.txt (line 49)) (1.0.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->-r requirement.txt (line 48)) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook->-r requirement.txt (line 48)) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook->-r requirement.txt (line 48)) (0.5.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers->-r requirement.txt (line 58))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->-r requirement.txt (line 48)) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook->-r requirement.txt (line 48)) (0.2.13)\n",
            "Building wheels for collected packages: cchardet, pycocotools, iopath, antlr4-python3-runtime, sentence-transformers, deepspeed, ffmpy\n",
            "  Building wheel for cchardet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cchardet: filename=cchardet-2.1.7-cp310-cp310-linux_x86_64.whl size=289403 sha256=ba602a8f7f0ddc2f25b474239196c95e0af8214db1412b7c6deaebc4e7c67af9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/e0/ab/e01326f15c59438d080b1496dbab8091e952ec72f35e3c437e\n",
            "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=377173 sha256=a9dd66b338c23f126694f304ab0f1ecc0afc95c8fe0c5a1b49c4be556f0dfcc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=0bd93cf69180ea4cfaca2b8f72f210bb4e23d2fd9e97c09b563f506bcbc3d4b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=797c875d9ddd1be6aca7945cb416f0a8e24831cabd38b80ccdce3c2cd63fb85e\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=87d849950198ee17be890d981d838727b5ea62dab2f923d5499373bf22a10941\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432240 sha256=a7c72f947ed61015439e352623b256509a66ccdc8e859136490c1bee4bfaa1fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/7c/43/bed44d8414c099ff962b754f425f7ff77cc623cc8a98e0da70\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=ae268ff62a88bfb51b82d09a81e8aaa5bfa41ea42e74b1e900b136dbe14ddc8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built cchardet pycocotools iopath antlr4-python3-runtime sentence-transformers deepspeed ffmpy\n",
            "Installing collected packages: tokenizers, pydub, ninja, lit, hjson, ffmpy, cchardet, braceexpand, bitsandbytes, antlr4-python3-runtime, addict, zipp, websockets, typer, tqdm, tenacity, smmap, setproctitle, sentry-sdk, semantic-version, scipy, regex, pyyaml, python-multipart, pyparsing, pynvml, pydantic, psutil, portalocker, pathlib-abc, packaging, orjson, opencv-python, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, mypy-extensions, multidict, markdown-it-py, kiwisolver, jedi, importlib-resources, imageio, h11, frozenlist, fonttools, filelock, einops, docker-pycreds, decord, cycler, contourpy, chardet, av, attrs, async-timeout, aiofiles, yarl, webdataset, uvicorn, typing-inspect, starlette, pathy, omegaconf, nvidia-cusolver-cu11, nvidia-cudnn-cu11, mdit-py-plugins, matplotlib, iopath, huggingface-hub, httpcore, gitdb, yapf, transformers, thinc, pyre-extensions, pynndescent, pycocotools, httpx, gradio-client, GitPython, fastapi, aiohttp, wandb, umap-learn, spacy, pycocoevalcap, openai, mmengine, gradio, triton, torch, torchvision, deepspeed, accelerate, xformers, torchaudio, timm, sentence-transformers, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.18.1\n",
            "    Uninstalling zipp-3.18.1:\n",
            "      Successfully uninstalled zipp-3.18.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.2\n",
            "    Uninstalling tqdm-4.66.2:\n",
            "      Successfully uninstalled tqdm-4.66.2\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.2.3\n",
            "    Uninstalling tenacity-8.2.3:\n",
            "      Successfully uninstalled tenacity-8.2.3\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2023.12.25\n",
            "    Uninstalling regex-2023.12.25:\n",
            "      Successfully uninstalled regex-2023.12.25\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.2\n",
            "    Uninstalling pyparsing-3.1.2:\n",
            "      Successfully uninstalled pyparsing-3.1.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.0\n",
            "    Uninstalling pydantic-2.7.0:\n",
            "      Successfully uninstalled pydantic-2.7.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.0.5\n",
            "    Uninstalling multidict-6.0.5:\n",
            "      Successfully uninstalled multidict-6.0.5\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.5\n",
            "    Uninstalling kiwisolver-1.4.5:\n",
            "      Successfully uninstalled kiwisolver-1.4.5\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib_resources 6.4.0\n",
            "    Uninstalling importlib_resources-6.4.0:\n",
            "      Successfully uninstalled importlib_resources-6.4.0\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.31.6\n",
            "    Uninstalling imageio-2.31.6:\n",
            "      Successfully uninstalled imageio-2.31.6\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.4.1\n",
            "    Uninstalling frozenlist-1.4.1:\n",
            "      Successfully uninstalled frozenlist-1.4.1\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.51.0\n",
            "    Uninstalling fonttools-4.51.0:\n",
            "      Successfully uninstalled fonttools-4.51.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.4\n",
            "    Uninstalling filelock-3.13.4:\n",
            "      Successfully uninstalled filelock-3.13.4\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.2.1\n",
            "    Uninstalling contourpy-1.2.1:\n",
            "      Successfully uninstalled contourpy-1.2.1\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.2.0\n",
            "    Uninstalling attrs-23.2.0:\n",
            "      Successfully uninstalled attrs-23.2.0\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 4.0.3\n",
            "    Uninstalling async-timeout-4.0.3:\n",
            "      Successfully uninstalled async-timeout-4.0.3\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.9.4\n",
            "    Uninstalling yarl-1.9.4:\n",
            "      Successfully uninstalled yarl-1.9.4\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.0\n",
            "    Uninstalling transformers-4.40.0:\n",
            "      Successfully uninstalled transformers-4.40.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.3\n",
            "    Uninstalling thinc-8.2.3:\n",
            "      Successfully uninstalled thinc-8.2.3\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.7\n",
            "    Uninstalling pycocotools-2.0.7:\n",
            "      Successfully uninstalled pycocotools-2.0.7\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.5\n",
            "    Uninstalling aiohttp-3.9.5:\n",
            "      Successfully uninstalled aiohttp-3.9.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.4\n",
            "    Uninstalling spacy-3.7.4:\n",
            "      Successfully uninstalled spacy-3.7.4\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.1+cu121\n",
            "    Uninstalling torchvision-0.17.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.17.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.2.1+cu121\n",
            "    Uninstalling torchaudio-2.2.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.2.0 requires matplotlib>=3.7.1, but you have matplotlib 3.7.0 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.5.1 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.43 accelerate-0.29.3 addict-2.4.0 aiofiles-23.2.1 aiohttp-3.8.4 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 attrs-22.2.0 av-11.0.0 bitsandbytes-0.37.0 braceexpand-0.1.7 cchardet-2.1.7 chardet-5.1.0 contourpy-1.0.7 cycler-0.11.0 decord-0.6.0 deepspeed-0.14.2 docker-pycreds-0.4.0 einops-0.7.0 fastapi-0.110.2 ffmpy-0.3.2 filelock-3.9.0 fonttools-4.38.0 frozenlist-1.3.3 gitdb-4.0.11 gradio-3.24.1 gradio-client-0.0.8 h11-0.14.0 hjson-3.1.0 httpcore-1.0.5 httpx-0.27.0 huggingface-hub-0.17.0 imageio-2.33.1 importlib-resources-5.12.0 iopath-0.1.10 jedi-0.19.1 kiwisolver-1.4.4 lit-18.1.3 markdown-it-py-2.2.0 matplotlib-3.7.0 mdit-py-plugins-0.3.3 mmengine-0.10.4 multidict-6.0.4 mypy-extensions-1.0.0 ninja-1.11.1.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 omegaconf-2.3.0 openai-0.27.0 opencv-python-4.7.0.72 orjson-3.10.1 packaging-23.0 pathlib-abc-0.1.1 pathy-0.11.0 peft-0.8.1 portalocker-2.8.2 psutil-5.9.4 pycocoevalcap-1.2 pycocotools-2.0.6 pydantic-1.10.15 pydub-0.25.1 pynndescent-0.5.12 pynvml-11.5.0 pyparsing-3.0.9 pyre-extensions-0.0.29 python-multipart-0.0.9 pyyaml-6.0 regex-2022.10.31 scipy-1.10.1 semantic-version-2.10.0 sentence-transformers-2.2.2 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 spacy-3.5.1 starlette-0.37.2 tenacity-8.2.2 thinc-8.1.12 timm-0.6.13 tokenizers-0.13.2 torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1 tqdm-4.64.1 transformers-4.28.0 triton-2.0.0 typer-0.7.0 typing-inspect-0.9.0 umap-learn-0.5.6 uvicorn-0.29.0 wandb-0.16.6 webdataset-0.2.48 websockets-12.0 xformers-0.0.19 yapf-0.40.2 yarl-1.8.2 zipp-3.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "psutil",
                  "pydevd_plugins"
                ]
              },
              "id": "685ba0a0414749618924f75d5b8fcd13"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/mapluisch/ST-LLM.git\n",
        "%cd ST-LLM\n",
        "!pip install -r requirement.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/ST-LLM/ST_Weights\n",
        "%cd /content/ST-LLM/ST_Weights\n",
        "!git init\n",
        "!git remote add origin https://huggingface.co/farewellthree/ST_LLM_weight\n",
        "!git fetch\n",
        "\n",
        "!git sparse-checkout init --cone\n",
        "!git sparse-checkout set conversation_weight\n",
        "\n",
        "!git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq29zuNWfIyZ",
        "outputId": "9438a133-e3c9-4531-b464-1f3ddd9190ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ST-LLM/ST_Weights\n",
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/ST-LLM/ST_Weights/.git/\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 31 (delta 5), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (31/31), 8.70 KiB | 891.00 KiB/s, done.\n",
            "From https://huggingface.co/farewellthree/ST_LLM_weight\n",
            " * [new branch]      main       -> origin/main\n",
            "From https://huggingface.co/farewellthree/ST_LLM_weight\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Filtering content: 100% (3/3), 3.00 GiB | 24.11 MiB/s, done.\n",
            "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
            "\tconversation_weight/pytorch_model-00002-of-00002.bin\n",
            "\tconversation_weight/pytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o /content/ST-LLM/blip2_pretrained_flant5xxl.pth https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xl.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iFlKyE6fkC0",
        "outputId": "3e76d8be-a2aa-44ea-84aa-46cb44b96bd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  407M  100  407M    0     0  75.5M      0  0:00:05  0:00:05 --:--:-- 85.0M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ST-LLM\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 demo_gradio.py --ckpt-path /content/ST-LLM/ST_Weights/conversation_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgFDGb8nflvl",
        "outputId": "8bf45d69-02a6-41cb-afb2-56ca263a450c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ST-LLM\n",
            "2024-04-24 14:33:52.966771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-24 14:33:52.966834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-24 14:33:52.968840: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-24 14:33:54.247467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Initializing Chat\n",
            "Loading checkpoint shards: 100% 2/2 [00:14<00:00,  7.18s/it]\n",
            "Some weights of the model checkpoint at /content/ST-LLM/ST_Weights/conversation_weight were not used when initializing STLLMForCausalLM: ['model.stllm_model.visual_encoder.blocks.36.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.23.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.37.norm1.bias', 'model.stllm_model.visual_encoder.blocks.36.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.4.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.13.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.38.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.8.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.6.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.25.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.36.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.18.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.self.query.weight', 'model.stllm_model.visual_encoder.blocks.38.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.13.norm1.bias', 'model.stllm_model.visual_encoder.blocks.0.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.8.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.36.norm1.weight', 'model.stllm_model.visual_encoder.blocks.10.norm2.weight', 'model.stllm_model.visual_encoder.blocks.0.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.30.norm1.bias', 'model.stllm_model.visual_encoder.blocks.5.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.17.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.4.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.0.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.10.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.16.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.3.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.30.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.10.norm1.bias', 'model.stllm_model.visual_encoder.blocks.17.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.19.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.27.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.22.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.20.norm2.weight', 'model.stllm_model.visual_encoder.blocks.32.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.16.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.self.key.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.self.query.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.embeddings.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.6.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.35.norm2.bias', 'model.stllm_model.visual_encoder.blocks.3.norm1.weight', 'model.stllm_model.visual_encoder.blocks.11.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.16.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.output.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.18.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.4.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.18.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.33.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.12.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.output_query.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.24.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.25.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.26.norm1.bias', 'model.stllm_model.visual_encoder.blocks.32.attn.q_bias', 'model.stllm_model.down_proj.weight', 'model.stllm_model.visual_encoder.blocks.14.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.self.query.weight', 'model.stllm_model.visual_encoder.blocks.3.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.2.attn.qkv.weight', 'model.stllm_model.visual_encoder.patch_embed.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.intermediate_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.15.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.23.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'model.stllm_model.down_proj.bias', 'model.stllm_model.visual_encoder.blocks.29.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.38.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.3.norm2.weight', 'model.stllm_model.visual_encoder.blocks.11.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.15.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.4.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.8.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.9.norm2.bias', 'model.stllm_model.visual_encoder.blocks.19.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.27.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.7.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.22.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.21.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.output_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.26.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.34.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.output_query.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.38.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.18.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.21.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.29.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.8.norm2.weight', 'model.stllm_model.up_proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.5.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.1.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.36.norm2.weight', 'model.stllm_model.visual_encoder.blocks.19.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.15.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.21.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.intermediate_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.10.norm1.weight', 'model.stllm_model.visual_encoder.blocks.11.norm1.weight', 'model.stllm_model.visual_encoder.blocks.14.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.25.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.intermediate_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.28.norm1.weight', 'model.stllm_model.visual_encoder.blocks.1.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.38.norm1.bias', 'model.stllm_model.visual_encoder.blocks.38.norm1.weight', 'model.stllm_model.visual_encoder.blocks.24.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.11.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.25.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.27.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.4.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.intermediate_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.37.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.2.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.24.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.22.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.self.key.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.self.value.weight', 'model.stllm_model.visual_encoder.pos_embed', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.27.norm2.weight', 'model.stllm_model.visual_encoder.blocks.23.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.output_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.27.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.25.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.0.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.13.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.1.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.5.norm2.bias', 'model.stllm_model.visual_encoder.blocks.12.norm2.bias', 'model.stllm_model.visual_encoder.blocks.22.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.18.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.25.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.self.key.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.output_query.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.2.norm1.weight', 'model.stllm_model.visual_encoder.blocks.2.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.output_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.37.norm2.weight', 'model.stllm_model.visual_encoder.blocks.23.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.18.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.25.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.38.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.34.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.output_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.17.norm2.bias', 'model.stllm_model.visual_encoder.blocks.23.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.36.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.36.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.7.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.21.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.7.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.output_query.LayerNorm.bias', 'model.stllm_model.Qformer.bert.embeddings.position_embeddings.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.29.norm1.weight', 'model.stllm_model.visual_encoder.blocks.15.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.30.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.7.norm1.bias', 'model.stllm_model.visual_encoder.blocks.5.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.8.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.self.value.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.self.key.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.24.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.25.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.29.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.38.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.30.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.19.norm2.weight', 'model.stllm_model.visual_encoder.blocks.20.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.24.norm1.bias', 'model.stllm_model.visual_encoder.blocks.5.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.24.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.37.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.17.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.22.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.31.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.34.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.self.query.weight', 'model.stllm_model.visual_encoder.blocks.14.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.intermediate_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.22.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.30.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.6.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.20.norm2.bias', 'model.stllm_model.visual_encoder.blocks.12.norm2.weight', 'model.stllm_model.visual_encoder.blocks.28.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.intermediate_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.31.norm2.bias', 'model.stllm_model.visual_encoder.blocks.33.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.9.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.10.norm2.bias', 'model.stllm_model.visual_encoder.blocks.30.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.10.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.16.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.31.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.8.norm1.bias', 'model.stllm_model.visual_encoder.blocks.33.norm1.weight', 'model.stllm_model.visual_encoder.blocks.26.norm2.weight', 'model.stllm_model.visual_encoder.blocks.11.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.intermediate_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.2.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.7.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.output_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.self.value.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.self.query.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.7.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.2.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.34.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.25.norm2.bias', 'model.stllm_model.visual_encoder.blocks.6.norm2.weight', 'model.stllm_model.llama_proj.weight', 'model.stllm_model.visual_encoder.blocks.5.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.self.value.weight', 'model.stllm_model.Qformer.bert.embeddings.word_embeddings.weight', 'model.stllm_model.visual_encoder.blocks.16.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.31.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.14.norm1.bias', 'model.stllm_model.visual_encoder.blocks.18.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.22.norm2.weight', 'model.stllm_model.visual_encoder.blocks.28.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.25.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.32.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.36.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.intermediate_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.17.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.output_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.19.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.30.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.output.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.output_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.27.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.6.norm1.bias', 'model.stllm_model.visual_encoder.blocks.24.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.intermediate_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.15.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.23.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.29.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.37.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.30.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.intermediate_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.26.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.17.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.10.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.14.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.18.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.self.key.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.33.norm2.weight', 'model.stllm_model.visual_encoder.blocks.38.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.35.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.33.norm2.bias', 'model.stllm_model.visual_encoder.blocks.27.norm2.bias', 'model.stllm_model.visual_encoder.blocks.0.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.6.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.12.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.7.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.output_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.intermediate_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.17.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.self.query.weight', 'model.stllm_model.visual_encoder.blocks.0.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.22.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.12.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.output_query.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.14.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.output_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.18.norm1.weight', 'model.stllm_model.visual_encoder.blocks.23.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.28.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.intermediate_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.12.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.1.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.10.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.12.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.33.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.17.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.34.norm2.bias', 'model.stllm_model.visual_encoder.blocks.15.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.7.output_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.37.norm2.bias', 'model.stllm_model.visual_encoder.blocks.28.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.33.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'model.stllm_model.up_proj.bias', 'model.stllm_model.visual_encoder.blocks.5.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.6.norm2.bias', 'model.stllm_model.visual_encoder.blocks.24.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.23.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.2.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.13.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.28.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.self.query.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.3.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.intermediate_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.1.norm2.weight', 'model.stllm_model.visual_encoder.blocks.9.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.29.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.34.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.21.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.1.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.8.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.intermediate.dense.bias', 'model.stllm_model.visual_encoder.blocks.9.norm1.weight', 'model.stllm_model.visual_encoder.blocks.32.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.23.norm1.bias', 'model.stllm_model.visual_encoder.blocks.22.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.16.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.self.query.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.intermediate_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.intermediate_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.6.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.24.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.15.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.21.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.31.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.37.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.20.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.5.norm1.weight', 'model.stllm_model.visual_encoder.blocks.30.norm2.bias', 'model.stllm_model.visual_encoder.blocks.15.norm2.weight', 'model.stllm_model.visual_encoder.blocks.13.norm2.bias', 'model.stllm_model.visual_encoder.blocks.37.norm1.weight', 'model.stllm_model.visual_encoder.blocks.9.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.15.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.intermediate_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.14.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.29.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.intermediate.dense.bias', 'model.stllm_model.visual_encoder.blocks.5.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.20.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.16.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.1.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.12.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.35.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.7.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.17.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.intermediate.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.intermediate.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.2.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.13.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.32.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.output_query.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.output_query.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.output_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.3.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.10.mlp.fc2.weight', 'model.stllm_model.query_tokens', 'model.stllm_model.visual_encoder.blocks.14.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.16.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.32.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.8.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.29.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.31.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.32.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.20.norm1.bias', 'model.stllm_model.visual_encoder.blocks.32.norm1.weight', 'model.stllm_model.visual_encoder.blocks.9.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.35.norm2.weight', 'model.stllm_model.visual_encoder.blocks.9.norm2.weight', 'model.stllm_model.visual_encoder.blocks.23.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.35.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.38.norm2.bias', 'model.stllm_model.visual_encoder.blocks.1.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.self.query.weight', 'model.stllm_model.visual_encoder.patch_embed.proj.weight', 'model.stllm_model.visual_encoder.blocks.19.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.11.norm1.bias', 'model.stllm_model.visual_encoder.blocks.21.norm1.bias', 'model.stllm_model.visual_encoder.blocks.23.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.4.norm2.bias', 'model.stllm_model.visual_encoder.blocks.10.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.25.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.4.norm2.weight', 'model.stllm_model.visual_encoder.blocks.13.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.19.norm2.bias', 'model.stllm_model.visual_encoder.blocks.37.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.35.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.35.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.19.norm1.weight', 'model.stllm_model.visual_encoder.blocks.37.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.20.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.26.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.34.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.1.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.38.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.7.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.13.norm2.weight', 'model.stllm_model.visual_encoder.blocks.36.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.crossattention.self.query.weight', 'model.stllm_model.visual_encoder.blocks.38.norm2.weight', 'model.stllm_model.visual_encoder.blocks.24.norm2.bias', 'model.stllm_model.visual_encoder.blocks.2.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.8.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.1.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.output_query.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.3.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.intermediate_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.self.value.weight', 'model.stllm_model.ln_vision.weight', 'model.stllm_model.visual_encoder.blocks.7.norm2.weight', 'model.stllm_model.visual_encoder.blocks.7.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.9.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.10.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.31.norm1.bias', 'model.stllm_model.visual_encoder.blocks.8.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.output_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.35.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.0.norm1.bias', 'model.stllm_model.visual_encoder.blocks.32.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.14.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.2.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.0.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.self.key.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.self.value.weight', 'model.stllm_model.Qformer.bert.embeddings.position_ids', 'model.stllm_model.Qformer.bert.encoder.layer.1.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.intermediate_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.18.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.27.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.26.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.12.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.1.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.28.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.19.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.18.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.output_query.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.20.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.33.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.9.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.34.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.28.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.14.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.26.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.14.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.35.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.31.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.17.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.22.norm2.bias', 'model.stllm_model.visual_encoder.blocks.4.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.25.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.intermediate.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.10.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.29.norm2.weight', 'model.stllm_model.visual_encoder.blocks.11.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.16.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.24.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.35.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.intermediate_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.output_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.0.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.output_query.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.self.query.weight', 'model.stllm_model.visual_encoder.blocks.15.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.38.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.36.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.11.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.6.norm1.weight', 'model.stllm_model.visual_encoder.blocks.29.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.intermediate_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.self.query.weight', 'model.stllm_model.ln_vision.bias', 'model.stllm_model.visual_encoder.blocks.0.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.5.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.23.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.self.key.weight', 'model.stllm_model.visual_encoder.blocks.4.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.5.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.3.norm2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.3.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.self.query.weight', 'model.stllm_model.visual_encoder.blocks.13.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.16.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.8.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.21.norm2.bias', 'model.stllm_model.visual_encoder.blocks.9.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.5.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.15.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.13.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.36.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.32.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.29.norm1.bias', 'model.stllm_model.visual_encoder.blocks.31.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.14.norm1.weight', 'model.stllm_model.visual_encoder.blocks.13.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.30.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.self.value.bias', 'model.stllm_model.embed_tokens.weight', 'model.stllm_model.visual_encoder.blocks.33.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.output_query.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.4.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.34.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.28.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.self.query.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.12.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.27.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.35.norm1.weight', 'model.stllm_model.visual_encoder.blocks.29.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.37.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.26.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.11.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.24.norm2.weight', 'model.stllm_model.visual_encoder.blocks.33.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.33.norm1.bias', 'model.stllm_model.visual_encoder.blocks.37.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.attention.self.query.weight', 'model.stllm_model.Qformer.bert.encoder.layer.5.output_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.8.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.9.attn.proj.weight', 'model.stllm_model.llama_proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.attention.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.0.norm2.bias', 'model.stllm_model.visual_encoder.blocks.12.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.14.norm2.weight', 'model.stllm_model.visual_encoder.blocks.15.norm1.bias', 'model.stllm_model.visual_encoder.blocks.31.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.27.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.output.dense.bias', 'model.stllm_model.visual_encoder.blocks.32.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.4.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.output_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.30.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.34.attn.proj.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.6.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.output_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.11.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.intermediate.dense.weight', 'model.stllm_model.visual_encoder.blocks.23.norm2.weight', 'model.stllm_model.visual_encoder.blocks.26.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.27.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.intermediate_query.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.crossattention.self.query.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.30.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.3.output_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.output.dense.bias', 'model.stllm_model.Qformer.bert.embeddings.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.28.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.15.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.13.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.19.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.31.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.self.key.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.9.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.17.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.18.norm2.bias', 'model.stllm_model.visual_encoder.blocks.3.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.1.norm2.bias', 'model.stllm_model.visual_encoder.blocks.37.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.2.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.22.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.intermediate.dense.bias', 'model.stllm_model.visual_encoder.blocks.6.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.21.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.intermediate_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.21.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.26.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.0.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.output.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.33.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.28.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.6.attention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.24.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.20.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.16.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.10.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.7.output_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.36.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.3.attn.proj.bias', 'model.stllm_model.visual_encoder.cls_token', 'model.stllm_model.visual_encoder.blocks.31.norm2.weight', 'model.stllm_model.visual_encoder.blocks.4.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.output_query.dense.bias', 'model.stllm_model.visual_encoder.blocks.22.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.22.mlp.fc1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.self.query.bias', 'model.stllm_model.visual_encoder.blocks.19.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.21.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.26.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.5.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.12.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.3.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.20.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.9.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.output.dense.weight', 'model.stllm_model.visual_encoder.blocks.11.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.26.norm2.bias', 'model.stllm_model.visual_encoder.blocks.17.norm1.weight', 'model.stllm_model.visual_encoder.blocks.16.norm1.bias', 'model.stllm_model.visual_encoder.blocks.20.norm1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.output.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.output_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.1.norm1.weight', 'model.stllm_model.visual_encoder.blocks.11.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.27.mlp.fc1.weight', 'model.stllm_model.Qformer.bert.encoder.layer.11.intermediate.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output_query.LayerNorm.bias', 'model.stllm_model.visual_encoder.blocks.35.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.8.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.32.norm2.weight', 'model.stllm_model.visual_encoder.blocks.2.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.26.norm1.weight', 'model.stllm_model.visual_encoder.blocks.27.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.intermediate_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.9.attention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.6.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.20.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.7.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.31.attn.qkv.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.attention.self.key.weight', 'model.stllm_model.Qformer.bert.encoder.layer.0.output_query.dense.weight', 'model.stllm_model.visual_encoder.blocks.34.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.0.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.19.attn.qkv.weight', 'model.stllm_model.visual_encoder.blocks.7.attn.q_bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.intermediate_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.3.attention.self.key.bias', 'model.stllm_model.visual_encoder.blocks.29.attn.q_bias', 'model.stllm_model.visual_encoder.blocks.34.norm2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.7.attention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.13.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.34.norm1.bias', 'model.stllm_model.Qformer.bert.encoder.layer.6.crossattention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.30.norm2.weight', 'model.stllm_model.visual_encoder.blocks.17.norm2.weight', 'model.stllm_model.visual_encoder.blocks.3.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.4.norm1.bias', 'model.stllm_model.visual_encoder.blocks.36.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.25.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.19.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.35.attn.v_bias', 'model.stllm_model.visual_encoder.blocks.18.mlp.fc2.bias', 'model.stllm_model.Qformer.bert.encoder.layer.9.output.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.28.attn.v_bias', 'model.stllm_model.Qformer.bert.encoder.layer.4.attention.self.value.bias', 'model.stllm_model.Qformer.bert.encoder.layer.5.attention.output.dense.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.intermediate.dense.bias', 'model.stllm_model.visual_encoder.blocks.6.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.10.output_query.dense.weight', 'model.stllm_model.Qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'model.stllm_model.Qformer.bert.encoder.layer.8.crossattention.self.value.bias', 'model.stllm_model.visual_encoder.blocks.11.attn.proj.weight', 'model.stllm_model.visual_encoder.blocks.28.attn.proj.bias', 'model.stllm_model.Qformer.bert.encoder.layer.7.output_query.LayerNorm.weight', 'model.stllm_model.visual_encoder.blocks.20.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.2.crossattention.self.value.weight', 'model.stllm_model.visual_encoder.blocks.21.mlp.fc2.weight', 'model.stllm_model.visual_encoder.blocks.32.mlp.fc2.bias', 'model.stllm_model.visual_encoder.blocks.7.attn.proj.bias', 'model.stllm_model.visual_encoder.blocks.33.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.16.mlp.fc1.weight', 'model.stllm_model.visual_encoder.blocks.2.mlp.fc2.weight', 'model.stllm_model.Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'model.stllm_model.residual_index', 'model.stllm_model.visual_encoder.blocks.12.mlp.fc1.bias', 'model.stllm_model.visual_encoder.blocks.21.mlp.fc2.bias']\n",
            "- This IS expected if you are initializing STLLMForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing STLLMForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of STLLMForCausalLM were not initialized from the model checkpoint at /content/ST-LLM/ST_Weights/conversation_weight and are newly initialized: ['model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading vocab.txt: 100% 232k/232k [00:00<00:00, 1.78MB/s]\n",
            "Downloading tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 335kB/s]\n",
            "Downloading config.json: 100% 570/570 [00:00<00:00, 3.13MB/s]\n",
            "Loading VIT\n",
            "100% 1.89G/1.89G [00:06<00:00, 295MB/s]\n",
            "Loading VIT Done\n",
            "Loading Q-Former\n",
            "Loading Q-Former Done\n",
            "Loading LLAMA\n",
            "Load BLIP2-LLM Checkpoint: /content/ST-LLM/ST_Weights/conversation_weight\n",
            "Initialization Finished\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://fb8a0c76a2bf19100b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
            "gr_video:  /tmp/efdf8873d7a37ee417eb03978914881630ad111b/cooking.mp4\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Conversation(system='Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, give your answer that best addresses the question.\\n', roles=('Human: ', 'Assistant: '), messages=[['Human: ', '<Video><ImageHere></Video> ###Human: describe the video ###Assistant: '], ['Assistant: ', 'In the video, two girls are shown mixing ingredients together in a bowl. They use a wooden spoon to stir the mixture and add more ingredients as they go. The girls continue to mix the ingredients together until they form a dough-like consistency. From the video, it appears that they are making some sort of baked good, such as cookies or muffins. The girls seem to be enjoying themselves as they work together to create something delicious. Overall, the video showcases the process of making baked goods from start to finish, with a focus on teamwork and the joy of cooking.</s>']], offset=2, instruction=False, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, skip_next=False, conv_id=None)\n",
            "Answer: In the video, two girls are shown mixing ingredients together in a bowl. They use a wooden spoon to stir the mixture and add more ingredients as they go. The girls continue to mix the ingredients together until they form a dough-like consistency. From the video, it appears that they are making some sort of baked good, such as cookies or muffins. The girls seem to be enjoying themselves as they work together to create something delicious. Overall, the video showcases the process of making baked goods from start to finish, with a focus on teamwork and the joy of cooking.</s>\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1793, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ST-LLM/demo_gradio.py\", line 207, in <module>\n",
            "    demo.launch(share=True, enable_queue=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1710, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1796, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 41, in close\n",
            "    def close(self):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://fb8a0c76a2bf19100b.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}